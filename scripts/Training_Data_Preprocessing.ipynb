{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## processing training data for deepcell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.io as io\n",
    "import importlib\n",
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from segmentation.utils import data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create npz array of labeled images for training\n",
    "base_dir = '/Users/noahgreenwald/Documents/Grad_School/Lab/Segmentation_Project/data/20191112_lab_combined/lab_combined_test_small/'\n",
    "data_name = \"training_freeze_1_H3_NaK\"\n",
    "\n",
    "# load data from previously created xarray\n",
    "training_data_x = xr.open_dataarray(base_dir + data_name + \"_X.nc\")\n",
    "training_data_y = xr.open_dataarray(base_dir + data_name + \"_y.nc\")\n",
    "\n",
    "# or create one now\n",
    "training_data_x = data_utils.load_tifs_from_points_dir(base_dir, tif_folder=\"raw\", tifs=[\"HH3.tif\", \"NaK ATPase.tif\"])\n",
    "io.imshow(training_data_x[2, :, :, 1])\n",
    "\n",
    "training_data_y = data_utils.load_tifs_from_points_dir(base_dir, tif_folder=\"annotated\", tifs=[\"Cell_Mask_Label.tif\"])\n",
    "io.imshow(training_data_y[2, :, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset data if only a portion will be used\n",
    "training_data_x = training_data_x[:, :, :396, :]\n",
    "training_data_y = training_data_y[:, :, :396, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add blank channels if missing from imaging run\n",
    "channel_order = [\"H3\", \"NaK ATPase\", \"Lamin AC\"]\n",
    "non_blank_channels = [\"H3\", \"NaK ATPase\"]\n",
    "training_data_x = data_utils.reorder_xarray_channels(channel_order=channel_order, channel_xr=training_data_x,\n",
    "                                                           non_blank_channels=non_blank_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate out points that will become test points\n",
    "training_data_x_test = training_data_x.loc[training_data_x.points == \"Point12\"]\n",
    "training_data_x = training_data_x.loc[training_data_x.points != \"Point12\"]\n",
    "training_data_y_test = training_data_y.loc[training_data_y.points == \"Point12\"]\n",
    "training_data_y = training_data_y.loc[training_data_y.points != \"Point12\"]\n",
    "\n",
    "np.savez(base_dir + data_name + \"_test.npz\", X=training_data_x_test, y=training_data_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop data to appropriate size\n",
    "crop_size = 256\n",
    "stride = 0.3\n",
    "training_data_x_cropped = data_utils.crop_image_stack(training_data_x, crop_size=crop_size, stride_fraction=stride)\n",
    "training_data_y_cropped = data_utils.crop_image_stack(training_data_y, crop_size=crop_size, stride_fraction=stride)\n",
    "\n",
    "if training_data_y_cropped.shape[:-1] != training_data_x_cropped.shape[:-1]:\n",
    "    raise ValueError(\"cropped arrays have different sizes\")\n",
    "else:\n",
    "    print(\"looks good\")\n",
    "\n",
    "np.savez(base_dir + data_name + \"_{}x{}_stride_{}.npz\".format(crop_size, crop_size, stride),\n",
    "         X=training_data_x_cropped, y=training_data_y_cropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine different npzs together\n",
    "npz1 = np.load(base_dir + \"R1_Point26_256x256_stride_0.3.npz\")\n",
    "npz2 = np.load(base_dir + \"R1_Point32_256x256_stride_0.3.npz\")\n",
    "npz3 = np.load(base_dir + \"R6_Point5_256x256_stride_0.3.npz\")\n",
    "\n",
    "combined_x = np.concatenate((npz1[\"X\"], npz2[\"X\"], npz3[\"X\"]), axis=0)\n",
    "combined_y = np.concatenate((npz1[\"y\"], npz2[\"y\"], npz3[\"y\"]), axis=0)\n",
    "\n",
    "np.savez(base_dir + \"PAH_Caliban_V3_redo.npz\", X=combined_x, y=combined_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segmentation",
   "language": "python",
   "name": "segmentation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
