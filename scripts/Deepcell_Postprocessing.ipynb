{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook takes the output of deepcell, processes it, segments cells, and outputs the extracted channel information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage.io as io\n",
    "import xarray as xr\n",
    "\n",
    "from segmentation.utils import data_utils, segmentation_utils, plot_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This script is currently configured as a template to run with the provided example data. If running your own data, make a copy of this notebook first before modifying it. If you change this script you'll get merge conflicts when updating to the latest version. Go to file-> make a copy to create a copy of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set up file paths\n",
    "base_dir = \"../data/example_dataset/\"\n",
    "input_dir = os.path.join(base_dir, \"input_data\")\n",
    "tiff_dir = os.path.join(input_dir, 'single_channel_inputs')\n",
    "label_dir = os.path.join(base_dir, 'deepcell_output')\n",
    "\n",
    "# points to look at (None for all)\n",
    "points = None\n",
    "\n",
    "# validate file paths (add extra paths to this list)\n",
    "data_utils.validate_paths([\n",
    "    base_dir,\n",
    "    input_dir,\n",
    "    label_dir,\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We compute the paths for the deepcell input TIFFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_input = os.listdir(input_dir)\n",
    "points_input = [p for p in points_input if p.split('.')[-1] in ['tif', 'tiff']]\n",
    "if points:\n",
    "    points_input = [p for p in points_input if p.split('_deepcell_input')[0] in points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can then load the segmented mask from deepcell via label-map TIFFs and save as an xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load TIFFs of segmentation labels (from pixel clustering, deepcell, etc.) and save in label_dir as xr\n",
    "segmentation_labels = data_utils.tiffs_to_xr_labels(tiff_dir=label_dir, output_dir=label_dir, delimiter='_feature_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also then save the segmented mask overlaid on the imaging data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# get input data for overlay\n",
    "input_data_xr = data_utils.load_imgs_from_multitiff([os.path.join(input_dir, point) for point in points_input])\n",
    "\n",
    "for fov in input_data_xr.fovs:\n",
    "    plot_utils.plot_overlay(segmentation_labels.loc[fov, :, :, \"whole_cell\"].values,\n",
    "                            input_data_xr.loc[fov, :, :, :].values,\n",
    "                            path=os.path.join(label_dir, f'{fov.values}_overlay.tif'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Afterwards, we can generate expression matrices from the labeling + imaging data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if loading your own dataset, make sure all imaging data is in the same folder, with each FOV given it's own folder\n",
    "# All FOVs must have the same channels\n",
    "\n",
    "# If the TIFs are in a subfolder, specify the name here\n",
    "img_sub_folder = \"TIFs\"\n",
    "\n",
    "if not points:\n",
    "    # load channel data\n",
    "    all_points = os.listdir(tiff_dir)\n",
    "    all_points = [point for point in all_points if os.path.isdir(os.path.join(tiff_dir, point))\n",
    "                  and point.startswith(\"Point\")]\n",
    "    points = all_points\n",
    "points.sort()\n",
    "\n",
    "single_cell_dir = base_dir + \"single_cell_output\"\n",
    "\n",
    "if not os.path.exists(single_cell_dir):\n",
    "    os.makedirs(single_cell_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if loading more data than can fit into memory at once, we loop through in smaller increments\n",
    "batch_size = 5\n",
    "cohort_len = len(points)\n",
    "num_batch = int(np.floor(cohort_len / batch_size))\n",
    "combined_normalized_data = pd.DataFrame()\n",
    "combined_transformed_data = pd.DataFrame()\n",
    "\n",
    "for i in range(num_batch):\n",
    "    current_points = points[i * batch_size:(i + 1) * batch_size]\n",
    "    image_data = data_utils.load_imgs_from_tree(data_dir=tiff_dir, img_sub_folder=img_sub_folder, \n",
    "                                                      fovs=current_points)\n",
    "    current_labels = segmentation_labels.loc[current_points, :, :, :]\n",
    "    \n",
    "    # segment the imaging data\n",
    "    normalized_data, transformed_data = segmentation_utils.generate_expression_matrix(segmentation_labels=current_labels, image_data=image_data)\n",
    "    \n",
    "    combined_normalized_data = combined_normalized_data.append(normalized_data)\n",
    "    combined_transformed_data = combined_transformed_data.append(transformed_data)\n",
    "# if batch did not divide evenly into total, process remainder\n",
    "if cohort_len % batch_size != 0:\n",
    "    current_points = points[num_batch * batch_size:]\n",
    "    image_data = data_utils.load_imgs_from_tree(data_dir=tiff_dir, img_sub_folder=img_sub_folder, fovs=current_points)\n",
    "    current_labels = segmentation_labels.loc[current_points, :, :, :]\n",
    "    \n",
    "    # segment the imaging data\n",
    "    normalized_data, transformed_data = segmentation_utils.generate_expression_matrix(segmentation_labels=current_labels, image_data=image_data)\n",
    "    \n",
    "    combined_normalized_data = combined_normalized_data.append(normalized_data)\n",
    "    combined_transformed_data = combined_transformed_data.append(transformed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output as CSV\n",
    "combined_normalized_data.to_csv(os.path.join(single_cell_dir, 'normalized_data.csv'), index=False)\n",
    "combined_transformed_data.to_csv(os.path.join(single_cell_dir, 'transformed_data.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}