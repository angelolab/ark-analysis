{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a notebook to format your data for segmentation, run the images through the cloud instance of Mesmer, and then extract marker counts and morphological information from all the cells in your images.  This version also exemplifies Google Drive interoperability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "import"
    ]
   },
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import os\n",
    "import warnings\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "from ark.utils import data_utils, deepcell_service_utils, io_utils, load_utils, io_utils, plot_utils, segmentation_utils\n",
    "from ark.utils.google_drive_utils import init_google_drive_api, GoogleDrivePath, path_join, drive_write_out, DriveOpen\n",
    "from ark.segmentation import marker_quantification"
   ]
  },
  {
   "source": [
    "### To use Google Drive with ark-analysis, you'll first have to initialize it via `init_google_drive_api` and a universal passcode.  To get access to this passcode and Google Drive, email someone  -\\\\\\_('\\_' )_/-"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_google_drive_api(\"passcode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All data, images, files, etc. must be placed in the 'data' directory, and referenced via '../data/path_to_your_data'.\n",
    "\n",
    "The syntax for creating a `GoogleDrivePath` is very similar to that of making a regular path string.  Some general usage tips:\n",
    " * Use `path_join` instead of `os.path.join` when combining filepaths\n",
    " * Use `parent / child` syntax when possible to visually differentiate Drive paths from local paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "file_path"
    ]
   },
   "outputs": [],
   "source": [
    "# set up file paths\n",
    "base_dir = GoogleDrivePath('/test_output')\n",
    "input_dir = base_dir / 'input_data'\n",
    "tiff_dir = input_dir / 'single_channel_inputs'\n",
    "\n",
    "#base_local_dir = '../data/gdrive_testouts'\n",
    "deepcell_input_dir = input_dir / \"deepcell_input_test\"\n",
    "deepcell_output_dir = base_dir / 'deepcell_output_test'\n",
    "single_cell_dir = base_dir / \"single_cell_output_test\"\n",
    "viz_dir = base_dir / \"deepcell_visualization_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create_dirs"
    ]
   },
   "outputs": [],
   "source": [
    "# create directories if do not exist\n",
    "for directory in [deepcell_input_dir, deepcell_output_dir, single_cell_dir, viz_dir]:\n",
    "    if type(directory) is GoogleDrivePath:\n",
    "        directory.mkdir()\n",
    "        print(directory.fileID)\n",
    "    elif not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To get access to the test data used here, email someone, and create a Drive shortcut in your root directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "validate_path"
    ]
   },
   "outputs": [],
   "source": [
    "# validate paths\n",
    "io_utils.validate_paths([base_dir,\n",
    "                         input_dir,\n",
    "                         tiff_dir,\n",
    "                         deepcell_input_dir,\n",
    "                         deepcell_output_dir,\n",
    "                         single_cell_dir,\n",
    "                         viz_dir\n",
    "                         ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute and filter fov paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "mibitiff_set"
    ]
   },
   "outputs": [],
   "source": [
    "# set this to true for multi-channel tiffs\n",
    "MIBItiff = False\n",
    "\n",
    "# data file suffix for low-level processed data\n",
    "# only needed for MIBItiff = True\n",
    "MIBItiff_suffix = \"-MassCorrected-Filtered.tiff\""
   ]
  },
  {
   "source": [
    "### We can remotely list all the fovs in our Google Drive folder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "load_fovs"
    ]
   },
   "outputs": [],
   "source": [
    "# either get all fovs in the folder...\n",
    "if MIBItiff:\n",
    "    fovs = io_utils.list_files(tiff_dir, substrs=MIBItiff_suffix)\n",
    "else:\n",
    "    fovs = io_utils.list_folders(tiff_dir)\n",
    "\n",
    "# ... or optionally, select a specific set of fovs manually\n",
    "# fovs = [\"fov1\", \"fov2\"]\n",
    "\n",
    "print(fovs)\n",
    "\n",
    "# TODO: MIBItiff manual selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nuc_mem_set"
    ]
   },
   "outputs": [],
   "source": [
    "# NOTE: at least one of nucs and mems must not be None\n",
    "# nuclear channel name(s) (or nucs = None)\n",
    "nucs = ['HH3']\n",
    "\n",
    "# membrane channel name(s) (or mems = None)\n",
    "mems = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "set_channels"
    ]
   },
   "outputs": [],
   "source": [
    "# load channels to be included in Mesmer data\n",
    "channels = (nucs if nucs else []) + (mems if mems else [])\n",
    "\n",
    "# filter channels for None (just in case)\n",
    "channels = [channel for channel in channels if channel is not None]"
   ]
  },
  {
   "source": [
    "### Here we download the required image data directly from Drive.  No image data is saved locally!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "load_data_xr"
    ]
   },
   "outputs": [],
   "source": [
    "if MIBItiff:\n",
    "    data_xr = load_utils.load_imgs_from_mibitiff(tiff_dir, mibitiff_files=fovs, channels=channels)\n",
    "else:\n",
    "    data_xr = load_utils.load_imgs_from_tree(tiff_dir, img_sub_folder=\"TIFs\", fovs=fovs, channels=channels)"
   ]
  },
  {
   "source": [
    "### Note: If `deepcell_input_dir` is a local drive, the inputs for deepcell will be saved locally.  However, if `deepcell_input_dir` is a `GoogleDrivePath`, then the inputs will be automatically saved to the specified Drive folder.\n",
    "\n",
    "### In general, it's best to save 'mid-processed' data locally to avoid upload/download slow downs, and only upload data directly if it's 'final'.  That advice is ignored here for demonstration purposes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "gen_input"
    ]
   },
   "outputs": [],
   "source": [
    "# generate and save deepcell input tifs\n",
    "data_utils.generate_deepcell_input(data_xr, deepcell_input_dir, nucs, mems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload files to Deepcell and download results\n",
    "\n",
    "Deepcell input images will be zipped into a single file, uploaded to [deepcell.org](https://deepcell.org),\n",
    "\n",
    "and the output will be downloaded to the deepcell output directory on Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create_output"
    ]
   },
   "outputs": [],
   "source": [
    "deepcell_service_utils.create_deepcell_output(deepcell_input_dir, deepcell_output_dir, fovs=fovs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can then load the segmented mask from deepcell via label-map TIFFs and save as an xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "load_seg_labels"
    ]
   },
   "outputs": [],
   "source": [
    "segmentation_labels_cell = load_utils.load_imgs_from_dir(data_dir=deepcell_output_dir,\n",
    "                                                    xr_dim_name='compartments',\n",
    "                                                    xr_channel_names=['whole_cell'],\n",
    "                                                    trim_suffix='_feature_0',\n",
    "                                                    match_substring='_feature_0',\n",
    "                                                    force_ints=True)\n",
    "\n",
    "segmentation_labels_nuc = load_utils.load_imgs_from_dir(data_dir=deepcell_output_dir,\n",
    "                                                    xr_dim_name='compartments',\n",
    "                                                    xr_channel_names=['nuclear'],\n",
    "                                                    trim_suffix='_feature_1',\n",
    "                                                    match_substring='_feature_1',\n",
    "                                                    force_ints=True)\n",
    "\n",
    "segmentation_labels = xr.DataArray(np.concatenate((segmentation_labels_cell.values,\n",
    "                                                  segmentation_labels_nuc.values),\n",
    "                                                  axis=-1),\n",
    "                                   coords=[segmentation_labels_cell.fovs, \n",
    "                                           segmentation_labels_cell.rows,\n",
    "                                           segmentation_labels_cell.cols,\n",
    "                                           ['whole_cell', 'nuclear']],\n",
    "                                   dims=segmentation_labels_cell.dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can now extract the segmented imaging data to create normalized and transformed expression matrices\n",
    "\n",
    "### Note: if you're loading your own dataset, please make sure all the imaging data is in the same folder with each fov given its own folder and all fovs having the same channels.\n",
    "\n",
    "For a full list of features extracted, please refer to the cell table section of: https://ark-analysis.readthedocs.io/en/latest/_rtd/data_types.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "create_exp_mat"
    ]
   },
   "outputs": [],
   "source": [
    "cell_table_size_normalized, cell_table_arcsinh_transformed = \\\n",
    "    marker_quantification.generate_cell_table(segmentation_labels=segmentation_labels,\n",
    "                                              tiff_dir=tiff_dir,\n",
    "                                              img_sub_folder=\"TIFs\",\n",
    "                                              is_mibitiff=MIBItiff,\n",
    "                                              fovs=fovs,\n",
    "                                              batch_size=5,\n",
    "                                              nuclear_counts=True)"
   ]
  },
  {
   "source": [
    "### Here we write out our expression matrices (aka cell tables) to Google Drive.  This syntax looks a little odd and warrants some explanation.\n",
    "\n",
    "With normal local paths, we'd simply run the command:\n",
    "```\n",
    "cell_table_arcsinh_transformed.to_csv(os.path.join(single_cell_dir, 'cell_table_archsinh_transformed.csv'), index=False)\n",
    "```\n",
    "or, generalized:\n",
    "```\n",
    "df.to_csv(my_path, *args, **kwargs)\n",
    "```\n",
    "\n",
    "In this case, we're relying on the pandas `DataFrame` method `to_csv` to write our data out.  Since pandas has no affiliation with ark-analysis, it doesn't know about `GoogleDrivePaths`, so it would be fairly upset if it was given `my_path = GoogleDrivePath('/some_path')` as an argument.\n",
    "\n",
    "---\n",
    "\n",
    "To deal with cases like these, we utilize the `drive_write_out` function/pattern.  We pass the GoogleDrivePath, as well as a lambda function, to accomplish the write out.  For the generalized example, this looks like the following:\n",
    "\n",
    "```\n",
    "drive_write_out( my_path, lambda x: df.to_csv(x, *args, **kwargs) )\n",
    "```\n",
    "\n",
    "Note that this `lambda` function could also be a regular function:\n",
    "```\n",
    "def to_csv_wrapper(x):\n",
    "    df.to_csv(x, *args, **kwargs)\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "save_exp_mat"
    ]
   },
   "outputs": [],
   "source": [
    "cell_table_arcsinh_path = path_join(single_cell_dir, 'cell_table_arcsinh_transformed.csv')\n",
    "cell_table_normalized_path = path_join(single_cell_dir, 'cell_table_size_normalized.csv')\n",
    "\n",
    "drive_write_out(\n",
    "    cell_table_normalized_path,\n",
    "    lambda x: cell_table_size_normalized.to_csv(x, index=False)\n",
    ")\n",
    "\n",
    "drive_write_out(\n",
    "    cell_table_arcsinh_path,\n",
    "    lambda x: cell_table_arcsinh_transformed.to_csv(x, index=False)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}